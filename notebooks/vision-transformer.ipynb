{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download a dataset and preview images\n",
    "\n",
    "A model is only as good as its dataset.\n",
    "\n",
    "Training tools need lots of high-quality data to build accurate models. We'll use the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) of 60,000 photos to build our image classifier. Get started by downloading the dataset with `torchvision` and previewing a handful of images from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Download the CIFAR-10 dataset to ./data\n",
    "batch_size=10\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "print(\"Downloading training data...\")\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "print(\"Downloading testing data...\")\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Our model will recognize these kinds of objects\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Grab images from our training data\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "for i in range(batch_size):\n",
    "    # Add new subplot\n",
    "    plt.subplot(2, int(batch_size/2), i + 1)\n",
    "    # Plot the image\n",
    "    img = images[i]\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    # Add the image's label\n",
    "    plt.title(classes[labels[i]])\n",
    "\n",
    "plt.suptitle('Preview of Training Data', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Configure the vision transformer\n",
    "\n",
    "Now that we have our dataset, we need to set up a vision transformer for PyTorch. Our vision transformer will transform an image into a description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def _init_vit_weights(m):\n",
    "    \"\"\"\n",
    "    ViT weight initialization\n",
    "    :param m: module\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.trunc_normal_(m.weight, std=.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.zeros_(m.bias)\n",
    "        nn.init.ones_(m.weight)\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    对2D图像作Patch Embedding操作\n",
    "    \"\"\"\n",
    "    def __int__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768):\n",
    "        \"\"\"\n",
    "        此函数用于初始化相关参数\n",
    "        :param img_size: 输入图像的大小\n",
    "        :param patch_size: 一个patch的大小\n",
    "        :param in_c: 输入图像的通道数\n",
    "        :param embed_dim: 输出的每个token的维度\n",
    "        :param norm_layer: 指定归一化方式，默认为None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        img_size = (img_size, img_size) # 224 -> (224, 224)\n",
    "        patch_size = (patch_size, patch_size) # 16 -> (16, 16)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = # 计算原始图像被划分为多少个小块\n",
    "        self.num_patches = # 计算patch的个数\n",
    "        # 定义卷积层\n",
    "        self.proj =\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        此函数用于前向传播\n",
    "        :param x: 原始图像\n",
    "        :return: 处理后的图像\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        # 检查图像高宽和预先设定是否一致，不一致则报错\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        # 对图像依次作卷积、展平和调换处理: [B, C, H, W] -> [B, C, HW] -> [B, HW, C]\n",
    "        x =\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, # 输入token的dim\n",
    "                 num_heads=8, qkv_bias=False,\n",
    "                 attn_drop_ratio=0., proj_drop_ratio=0.):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim =\n",
    "        self.scale =\n",
    "        self.qkv =\n",
    "        self.attn_drop = nn.Dropout(attn_drop_ratio)\n",
    "        self.proj =\n",
    "        self.proj_drop = nn.Dropout(proj_drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch_size, num_patches + 1, total_embed_dim]\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim]\n",
    "        # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]\n",
    "        # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        qkv =\n",
    "        # [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1]\n",
    "        attn =\n",
    "        attn =\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]\n",
    "        # reshape: -> [batch_size, num_patches + 1, total_embed_dim]\n",
    "        x =\n",
    "        x =\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = in_features\n",
    "        hidden_features = hidden_features\n",
    "        self.fc1 =\n",
    "        self.act =\n",
    "        self.fc2 =\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x =\n",
    "        x =\n",
    "        x = self.drop(x)\n",
    "        x =\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop_ratio=0.,\n",
    "                 attn_drop_ratio=0., drop_path_ratio=0., norm_layer=nn.LayerNorm):\n",
    "        super(Block, self).__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn =\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp =\n",
    "\n",
    "    def forward(self, x):\n",
    "        x =\n",
    "        x =\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define a vision transformer\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000,\n",
    "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True,\n",
    "                 drop_ratio=0., attn_drop_ratio=0., drop_path_ratio=0., embed_layer=PatchEmbed):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_c (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            drop_ratio (float): dropout rate\n",
    "            attn_drop_ratio (float): attention dropout rate\n",
    "            drop_path_ratio (float): stochastic depth rate\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim # num_features for consistency with other models\n",
    "        self.num_tokens = 1\n",
    "        norm_layer = partial(nn.LayerNorm, eps=1e-6)\n",
    "\n",
    "        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        # 产生一个类别向量，维度和patch embedding一致\n",
    "        self.cls_token =\n",
    "        # 产生一个位置向量，长度和patch embedding一致\n",
    "        self.pos_embed =\n",
    "        self.pos_drop = nn.Dropout(p=drop_ratio)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)] # Block的drop_path_ratio参数\n",
    "        self.blocks =\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head(s)\n",
    "        self.head =\n",
    "\n",
    "        # Weight init\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        \n",
    "        # Weight init\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        \n",
    "        self.apply(_init_vit_weights)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        # [B, C, H, W] -> [B, num_patches, embed_dim]\n",
    "        x = self.patch_embed(x)  # [B, 196, 768]\n",
    "        # [1, 1, 768] -> [B, 1, 768]\n",
    "        cls_token =\n",
    "        # [B, 1, 768], [B, 196, 768] -> [B, 197, 768]\n",
    "        x =\n",
    "\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x =\n",
    "        return x\n",
    "net = VisionTransformer(img_size=32, patch_size=4, embed_dim=512, depth=12, num_heads=16, num_classes=10)\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "print(\"Your model is ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train the model and save model\n",
    "\n",
    "PyTorch trains our model by adjusting its parameters and evaluating its performance against our labelled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 10\n",
    "print(\"Training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(tqdm(trainloader, desc=f\"Epoch {epoch + 1} of {EPOCHS}\", leave=True, ncols=80)):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save our trained model\n",
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Test the trained model\n",
    "\n",
    "Let's test our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick random photos from training set\n",
    "if dataiter == None:\n",
    "    dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Load our model\n",
    "net = VisionTransformer()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# Analyze images\n",
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Show results\n",
    "for i in range(batch_size):\n",
    "    # Add new subplot\n",
    "    plt.subplot(2, int(batch_size/2), i + 1)\n",
    "    # Plot the image\n",
    "    img = images[i]\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    # Add the image's label\n",
    "    color = \"green\"\n",
    "    label = classes[predicted[i]]\n",
    "    if classes[labels[i]] != classes[predicted[i]]:\n",
    "        color = \"red\"\n",
    "        label = \"(\" + label + \")\"\n",
    "    plt.title(label, color=color)\n",
    "\n",
    "plt.suptitle('Objects Found by Model', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Evaluate model accuracy\n",
    "\n",
    "Let's conclude by evaluating our model's overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure accuracy for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "# Print accuracy statistics\n",
    "average_accuray = []\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    average_accuray.append(accuracy)\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
    "print ('Average Acc:', np.mean(average_accuray))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
